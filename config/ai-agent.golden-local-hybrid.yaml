# ═══════════════════════════════════════════════════════════════════════════
# Golden Baseline #3: Local Hybrid Pipeline - Privacy-Focused Configuration
# ═══════════════════════════════════════════════════════════════════════════
#
# VALIDATED CONFIGURATION - Production-ready
#
# Architecture: Hybrid Cloud-Local Pipeline
#   STT: Vosk (local)   - Audio privacy, no cloud transmission of voice
#   LLM: OpenAI (cloud) - Fast, intelligent responses via API
#   TTS: Piper (local)  - Natural voice synthesis, offline capability
#
# Requirements:
#   - OPENAI_API_KEY in .env file (for LLM only)
#   - 8GB+ RAM recommended (16GB optimal)
#   - Docker with local-ai-server container running
#   - First start downloads ~200MB models (5-10 minutes)
#   - Shared storage: /mnt/asterisk_media/ai-generated
#
# Performance:
#   - Response time: 3-7 seconds (typical on modern hardware)
#   - Audio quality: Very good (Piper TTS, local Vosk STT)
#   - Cost: ~$0.001-0.003 per minute (only LLM charged)
#
# Best for:
#   - Audio privacy compliance (HIPAA, GDPR scenarios)
#   - Cost-sensitive deployments (90% cheaper than full cloud)
#   - Hybrid cloud-local infrastructure
#   - Organizations with cloud policy restrictions
#   - Scenarios requiring audit trails for voice data
#
# For detailed parameter explanations, see: docs/Configuration-Reference.md
# ═══════════════════════════════════════════════════════════════════════════

# Pipeline Definition
# Modular pipeline: separate providers for each stage (STT → LLM → TTS)
pipelines:
  local_hybrid:
    stt: local_stt                      # Vosk speech-to-text (local-ai-server)
    llm: openai_llm                     # OpenAI LLM (cloud API)
    tts: local_tts                      # Piper text-to-speech (local-ai-server)
    options:
      stt:
        chunk_ms: 320                   # Audio chunk size for STT processing
                                        # Range: 160-640ms. Higher = more context, higher latency
        streaming: true                 # Enable streaming STT (real-time transcription)
        stream_format: "pcm16_16k"      # Internal audio format (PCM16 @ 16kHz)
      llm:
        base_url: "https://api.openai.com/v1"  # OpenAI API endpoint
        model: "gpt-4o-mini"            # LLM model (fast and cost-effective)
                                        # Options: gpt-4o-mini, gpt-4o, gpt-3.5-turbo
        temperature: 0.7                # Response creativity (0.0-2.0)
                                        # Lower = more focused, higher = more creative
        max_tokens: 150                 # Maximum response length
                                        # Range: 50-500. Higher = longer responses, higher cost
      tts:
        format:
          encoding: ulaw                # Output format: μ-law for telephony
          sample_rate: 8000             # Output sample rate

# Active Pipeline Selection
# Set which pipeline to use (must match a key in pipelines: above)
active_pipeline: local_hybrid

# Audio Transport
# "externalmedia" = RTP-based (recommended for pipelines, better compatibility)
# "audiosocket" = TCP-based (also supported)
audio_transport: "externalmedia"

# Playback Mode
# "file" = File-based playback (REQUIRED for pipelines)
# Pipelines generate audio files that Asterisk plays back
downstream_mode: "file"

# AudioSocket Configuration (alternative transport)
# Use if you prefer AudioSocket over ExternalMedia RTP
audiosocket:
  host: "0.0.0.0"                       # Bind to all interfaces
  port: 8090                            # Standard AudioSocket port
  format: "ulaw"                        # Audio format: ulaw (8kHz telephony)

# ExternalMedia RTP Configuration (default transport)
# Recommended for pipeline configurations
external_media:
  rtp_host: "0.0.0.0"                   # Bind to all interfaces
  rtp_port: 18080                       # RTP listening port
  port_range: "18080:18099"             # Port range for multiple concurrent calls
                                        # Each call uses one port in this range
  codec: "ulaw"                         # RTP codec: ulaw (8kHz telephony standard)
  direction: "both"                     # Bidirectional audio (send + receive)

# Barge-In Configuration
# Controls when user speech can interrupt AI responses
barge_in:
  enabled: true                             # Enable interruption detection
  initial_protection_ms: 400                # Protect first 400ms of greeting from echo
                                            # Range: 200-600ms. Higher = less echo sensitivity
  min_ms: 400                               # Minimum sustained speech to trigger interrupt
                                            # Range: 250-600ms. Lower = more responsive
  energy_threshold: 1800                    # RMS energy threshold for speech detection
                                            # Range: 1000-3000. Lower = more sensitive
  cooldown_ms: 1000                         # Ignore new interrupts for this duration
                                            # Range: 500-1500ms. Prevents rapid re-triggering
  post_tts_end_protection_ms: 250           # Guard time after AI finishes speaking
                                            # Range: 250-500ms. Prevents clipping user response

# Streaming Configuration
# Controls audio buffering and streaming behavior (used for file playback)
streaming:
  sample_rate: 8000                         # Output sample rate (8kHz telephony)
  jitter_buffer_ms: 100                     # Buffer size for network jitter tolerance
                                            # Range: 60-200ms. Higher = more robust to jitter
  keepalive_interval_ms: 5000               # TCP keepalive interval
  connection_timeout_ms: 10000              # Max time to wait for connection
  fallback_timeout_ms: 8000                 # No audio for this long = fallback
  chunk_size_ms: 20                         # Audio chunk duration (20ms standard)
  min_start_ms: 300                         # Warm-up buffer before first audio frame
                                            # Range: 200-500ms. Higher = fewer underruns
  low_watermark_ms: 200                     # Minimum buffer level before refilling
                                            # Range: 100-300ms. Higher = more robust
  provider_grace_ms: 500                    # Tolerance for late provider chunks
  logging_level: "info"                     # Logging verbosity: debug, info, warning, error

# Voice Activity Detection (VAD)
# Detects when user is speaking vs silence
vad:
  enabled: true                             # Enable VAD for utterance segmentation
  mode: "webrtc"                            # VAD engine: webrtc (Google WebRTC VAD)
  webrtc_aggressiveness: 2                  # Silence detection sensitivity (0-3)
                                            # 0 = Least aggressive (keeps more audio)
                                            # 1 = Low aggressiveness
                                            # 2 = Normal aggressiveness (recommended)
                                            # 3 = Most aggressive (strips more silence)
  min_speech_duration_ms: 250               # Minimum speech length to process
                                            # Range: 100-500ms. Filters very short noises
  silence_duration_ms: 700                  # Silence duration to finalize utterance
                                            # Range: 500-1500ms. Lower = faster turns
  speech_pad_ms: 300                        # Padding around detected speech
                                            # Range: 100-500ms. Prevents word clipping
  max_utterance_duration_s: 30              # Maximum single utterance length
                                            # Range: 10-60s. Prevents runaway processing

# Provider Configurations
providers:
  # Local AI Server (Vosk STT + Piper TTS)
  # Runs in local-ai-server Docker container
  local:
    enabled: true                           # Enable local AI server
    ws_url: "${LOCAL_WS_URL:-ws://local-ai-server:8765}"
                                            # WebSocket URL for local-ai-server
                                            # Default uses Docker service name
                                            # Can override with LOCAL_WS_URL env var
    connect_timeout_sec: 5.0                # Connection timeout
                                            # Range: 2-10s. Increase for slow networks
    response_timeout_sec: 10.0              # Response timeout
                                            # Range: 5-30s. Increase for slow CPUs
    chunk_ms: 320                           # Audio chunk size for local processing
    
  # OpenAI (for LLM only in this configuration)
  # Cloud-based language model for intelligence
  openai:
    enabled: true                           # Enable OpenAI provider
    api_key: "${OPENAI_API_KEY}"            # API key from .env (NEVER hardcode!)
    
  # OpenAI Realtime (disabled - not used in pipeline mode)
  openai_realtime:
    enabled: false                          # Disabled for pipeline configuration
    
  # Deepgram (disabled - not used in this configuration)
  deepgram:
    enabled: false                          # Disabled for pipeline configuration

# LLM Configuration (Default/Fallback)
# Persona and greeting for the AI assistant
llm:
  initial_greeting: "Hello, how can I help you today?"
                                            # Default greeting (used if no provider override)
                                            # Can override per-call with AI_GREETING channel variable
  prompt: "You are a concise and helpful voice assistant. Keep replies under 20 words unless asked for detail."
                                            # Default persona/instructions
                                            # Can override per-call with AI_PERSONA channel variable
  model: "gpt-4o-mini"                      # Default model (overridden by pipeline.llm.model)

# Application Metadata
app_name: "ai-voice-agent"                  # Application name (for logging/tracking)
app_instance_id: "${APP_INSTANCE_ID:-default}"
                                            # Instance identifier (for multi-instance deployments)

# ═══════════════════════════════════════════════════════════════════════════
# Performance Characteristics
# ═══════════════════════════════════════════════════════════════════════════
#
# Component Latencies (typical):
#   - STT (Vosk):          100-200ms (local processing, real-time)
#   - LLM (GPT-4o-mini):   500-1500ms (cloud API call)
#   - TTS (Piper):         200-400ms (local synthesis)
#   - Total Turn Time:     3-7 seconds (sum of above + network)
#
# Cost Analysis:
#   - STT: FREE (local Vosk processing, no API charges)
#   - LLM: $0.001-0.003/minute (OpenAI API, only component charged)
#   - TTS: FREE (local Piper synthesis, no API charges)
#   - Total: ~$0.002/minute average (90% cheaper than full cloud)
#
# Hardware Requirements:
#   - CPU: Modern 4+ cores (2020+ CPUs recommended)
#          Intel i5-10400, AMD Ryzen 5 5600X or newer
#   - RAM: 8GB minimum, 16GB recommended
#          Vosk models: ~500MB, Piper models: ~100MB, overhead: ~500MB
#   - Disk: ~1GB for models and audio file cache
#   - Network: Stable internet for LLM API calls (low bandwidth, <10kbps)
#
# Privacy Benefits:
#   ✅ Voice audio NEVER leaves your infrastructure (STT/TTS local)
#   ✅ Only text transcripts sent to cloud (for LLM processing)
#   ✅ HIPAA/GDPR compliant audio handling (no PHI/PII in cloud audio)
#   ✅ Audit trail capability (log all cloud requests)
#   ✅ Air-gap compatible (can switch to local-only LLM if needed)
#
# Scalability:
#   - Concurrent calls: Limited by CPU/RAM (typically 5-20 per server)
#   - Bottleneck: Local-ai-server processing (STT/TTS)
#   - Scaling: Horizontal (add more servers) or vertical (upgrade CPU)
#
# ═══════════════════════════════════════════════════════════════════════════
